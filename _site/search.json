[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "강화학습 (2023)",
    "section": "",
    "text": "질문하는 방법\n\n카카오톡: 질문하러 가기 // 학기종료이후 폐쇄함\n이메일: guebin@jbnu.ac.kr\n직접방문: 자연과학대학 본관 205호\nZoom: 카카오톡이나 이메일로 미리 시간을 정할 것\nLMS쪽지: https://ieilms.jbnu.ac.kr/\n\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 1, 2023\n\n\n강화학습 (3) – LunarLander\n\n\n최규빈 \n\n\n\n\nAug 30, 2023\n\n\n강화학습 (2) – 4x4 grid\n\n\n최규빈 \n\n\n\n\nAug 29, 2023\n\n\n강화학습 (1) – bandit\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/A1.html",
    "href": "posts/A1.html",
    "title": "강화학습 (1) – bandit",
    "section": "",
    "text": "강의영상\n\n\n\n환경셋팅\n- 설치 (코랩)\n!pip install -q swig\n!pip install gymnasium\n!pip install gymnasium[box2d]\n\n\nimports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nref: https://gymnasium.farama.org/index.html\n\n\n\nintro\n- 강화학습(대충설명): 어떠한 “(게임)환경”이 있을때 거기서 “뭘 할지”를 학습하는 과업\n- 딥마인드: breakout \\(\\to\\) 알파고\n\nhttps://www.youtube.com/watch?v=TmPfTpjtdgg\n\n- 강화학습 미래? (이거 잘하면 먹고 살 수 있을까?)\n- 선행 (강화학습)\n\n프로그래밍 지식: 파이썬, 클래스에 대한 이해 // https://guebin.github.io/PP2023/ 10wk-2 이후\n딥러닝 기본지식: DNN // https://guebin.github.io/DL2022/ 3wk-02 ~ 4wk-02\n수학적인 지식: 마코프과정\n\n\n\nGame1: bandit\n- 문제설명: 두 개의 버튼이 있다. 버튼0을 누르면 1의 보상을, 버튼1을 누르면 100의 보상을 준다고 가정\n- 처음에 어떤 행동을 해야 하는가? —&gt; ??? 처음에는 아는게 없음 —&gt; 일단 “아무거나” 눌러보자.\n- 버튼을 아무거나 누르는 함수를 구현해보자.\n\naction_space = ['button0', 'button1'] \naction = np.random.choice(action_space)\naction\n\n'button0'\n\n\n- 보상을 주는 함수를 구현해보자.\n\nif action == 'button0': # button0을 눌렀다면 \n    reward = 1 \nelse: # button1을 눌렀다면 \n    reward = 100 \n\n\nreward\n\n1\n\n\n- 아무버튼이나 10번정도 눌러보면서 데이터를 쌓아보자.\n\nfor _ in range(10):\n    action = np.random.choice(action_space)\n    if action == 'button0': \n        reward = 1 \n    else: \n        reward = 100     \n    print(action,reward) \n\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton0 1\nbutton1 100\nbutton1 100\nbutton0 1\n\n\n- 깨달았음: button0을 누르면 1점을 받고, button1을 누르면 100점을 받는 “환경”이구나? \\(\\to\\) button1을 누르는 “동작”을 해야하는 상황이구나?\n\n여기에서 \\(\\to\\)의 과정을 체계화 시킨 학문이 강화학습\n\n\nfor _ in range(10):\n    action = action_space[1]\n    if action == 'button0': \n        reward = 1 \n    else: \n        reward = 100     \n    print(action,reward) \n\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\nbutton1 100\n\n\n\n게임 클리어\n\n- 강화학습: 환경을 이해 \\(\\to\\) 행동을 결정\n위의 과정이 잘 되었다는 의미로 사용하는 문장들\n\n강화학습이 성공적으로 잘 되었다.\n에이전트가 환경의 과제를 완료했다.\n에이전트가 환경에서 성공적으로 학습했다.\n에이전트가 올바른 행동을 학습했다.\n게임 클리어 (비공식)\n\n- 게임이 클리어 되었다는 것을 의미하는 지표를 정하고 싶다.\n\n첫 생각: button1을 누르는 순간 게임클리어로 보면 되지 않나?\n두번째 생각: 아니지? 우연히 누를수도 있잖아?\n게임클리어조건: 최근 20번의 보상이 1900점 이상이면 게임이 클리어 되었다고 생각하자.1\n\n1 button1을 눌러야 하는건 맞지만 20번에 한번정도의 실수는 눈감아 주는 조건- 무지한자 – 게임을 클리어할 수 없다.\n\naction_space = [0,1]\nrewards = [] \nfor t in range(50): # 10000번을 해도 못깸 \n    action = np.random.choice(action_space) # 무지한자의 행동 (찍어) \n    if action == 0: \n        reward = 1 \n        rewards.append(reward)\n    else: \n        reward = 100\n        rewards.append(reward)\n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {action}\\t\"\n        f\"reward= {reward}\\t\"\n        f\"reward20= {sum(rewards[-20:])}\\t\"\n    )\n    if np.sum(rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 1   reward= 100 reward20= 100   \nn_try = 2   action= 0   reward= 1   reward20= 101   \nn_try = 3   action= 0   reward= 1   reward20= 102   \nn_try = 4   action= 0   reward= 1   reward20= 103   \nn_try = 5   action= 1   reward= 100 reward20= 203   \nn_try = 6   action= 1   reward= 100 reward20= 303   \nn_try = 7   action= 1   reward= 100 reward20= 403   \nn_try = 8   action= 0   reward= 1   reward20= 404   \nn_try = 9   action= 1   reward= 100 reward20= 504   \nn_try = 10  action= 1   reward= 100 reward20= 604   \nn_try = 11  action= 0   reward= 1   reward20= 605   \nn_try = 12  action= 0   reward= 1   reward20= 606   \nn_try = 13  action= 1   reward= 100 reward20= 706   \nn_try = 14  action= 0   reward= 1   reward20= 707   \nn_try = 15  action= 0   reward= 1   reward20= 708   \nn_try = 16  action= 0   reward= 1   reward20= 709   \nn_try = 17  action= 1   reward= 100 reward20= 809   \nn_try = 18  action= 1   reward= 100 reward20= 909   \nn_try = 19  action= 0   reward= 1   reward20= 910   \nn_try = 20  action= 1   reward= 100 reward20= 1010  \nn_try = 21  action= 1   reward= 100 reward20= 1010  \nn_try = 22  action= 0   reward= 1   reward20= 1010  \nn_try = 23  action= 0   reward= 1   reward20= 1010  \nn_try = 24  action= 1   reward= 100 reward20= 1109  \nn_try = 25  action= 1   reward= 100 reward20= 1109  \nn_try = 26  action= 0   reward= 1   reward20= 1010  \nn_try = 27  action= 1   reward= 100 reward20= 1010  \nn_try = 28  action= 1   reward= 100 reward20= 1109  \nn_try = 29  action= 1   reward= 100 reward20= 1109  \nn_try = 30  action= 1   reward= 100 reward20= 1109  \nn_try = 31  action= 0   reward= 1   reward20= 1109  \nn_try = 32  action= 0   reward= 1   reward20= 1109  \nn_try = 33  action= 0   reward= 1   reward20= 1010  \nn_try = 34  action= 0   reward= 1   reward20= 1010  \nn_try = 35  action= 1   reward= 100 reward20= 1109  \nn_try = 36  action= 0   reward= 1   reward20= 1109  \nn_try = 37  action= 1   reward= 100 reward20= 1109  \nn_try = 38  action= 1   reward= 100 reward20= 1109  \nn_try = 39  action= 0   reward= 1   reward20= 1109  \nn_try = 40  action= 1   reward= 100 reward20= 1109  \nn_try = 41  action= 1   reward= 100 reward20= 1109  \nn_try = 42  action= 1   reward= 100 reward20= 1208  \nn_try = 43  action= 1   reward= 100 reward20= 1307  \nn_try = 44  action= 1   reward= 100 reward20= 1307  \nn_try = 45  action= 1   reward= 100 reward20= 1307  \nn_try = 46  action= 1   reward= 100 reward20= 1406  \nn_try = 47  action= 1   reward= 100 reward20= 1406  \nn_try = 48  action= 1   reward= 100 reward20= 1406  \nn_try = 49  action= 1   reward= 100 reward20= 1406  \nn_try = 50  action= 1   reward= 100 reward20= 1406  \n\n\n- 깨달은자 – 게임클리어\n\naction_space = [0,1]\nrewards = [] \nfor t in range(50): # 10000번을 해도 못깸 \n    #action = np.random.choice(action_space) # 무지한자의 행동 (찍어) \n    action = 1\n    if action == 0: \n        reward = 1 \n        rewards.append(reward)\n    else: \n        reward = 100\n        rewards.append(reward)\n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {action}\\t\"\n        f\"reward= {reward}\\t\"\n        f\"reward20= {sum(rewards[-20:])}\\t\"\n    )\n    if np.sum(rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 1   reward= 100 reward20= 100   \nn_try = 2   action= 1   reward= 100 reward20= 200   \nn_try = 3   action= 1   reward= 100 reward20= 300   \nn_try = 4   action= 1   reward= 100 reward20= 400   \nn_try = 5   action= 1   reward= 100 reward20= 500   \nn_try = 6   action= 1   reward= 100 reward20= 600   \nn_try = 7   action= 1   reward= 100 reward20= 700   \nn_try = 8   action= 1   reward= 100 reward20= 800   \nn_try = 9   action= 1   reward= 100 reward20= 900   \nn_try = 10  action= 1   reward= 100 reward20= 1000  \nn_try = 11  action= 1   reward= 100 reward20= 1100  \nn_try = 12  action= 1   reward= 100 reward20= 1200  \nn_try = 13  action= 1   reward= 100 reward20= 1300  \nn_try = 14  action= 1   reward= 100 reward20= 1400  \nn_try = 15  action= 1   reward= 100 reward20= 1500  \nn_try = 16  action= 1   reward= 100 reward20= 1600  \nn_try = 17  action= 1   reward= 100 reward20= 1700  \nn_try = 18  action= 1   reward= 100 reward20= 1800  \nn_try = 19  action= 1   reward= 100 reward20= 1900  \n\n\n\n\n수정1: action_space의 수정\n\naction_space = gym.spaces.Discrete(2)\naction_space\n\nDiscrete(2)\n\n\n- 좋은점1: sample\n\nfor _ in range(10):\n    print(action_space.sample())\n\n1\n1\n0\n0\n0\n0\n0\n1\n1\n1\n\n\n- 좋은점2: in\n\n0 in action_space # 유효한 액션을 검사 -- 0은 유효한 액션\n\nTrue\n\n\n\n1 in action_space # 유효한 액션을 검사 -- 1은 유효한 액션 \n\nTrue\n\n\n\n2 in action_space # 유효한 액션을 검사 -- 2는 유효하지 않은 액션 \n\nFalse\n\n\n- 코드 1차수정\n\naction_space = gym.spaces.Discrete(2) \nrewards = [] \nfor t in range(50): \n    action = action_space.sample()\n    #action = 1\n    if action == 0: \n        reward = 1 \n        rewards.append(reward)\n    else: \n        reward = 100\n        rewards.append(reward)\n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {action}\\t\"\n        f\"reward= {reward}\\t\"\n        f\"reward20= {sum(rewards[-20:])}\\t\"\n    )\n    if np.sum(rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 0   reward= 1   reward20= 1 \nn_try = 2   action= 0   reward= 1   reward20= 2 \nn_try = 3   action= 1   reward= 100 reward20= 102   \nn_try = 4   action= 1   reward= 100 reward20= 202   \nn_try = 5   action= 1   reward= 100 reward20= 302   \nn_try = 6   action= 0   reward= 1   reward20= 303   \nn_try = 7   action= 0   reward= 1   reward20= 304   \nn_try = 8   action= 0   reward= 1   reward20= 305   \nn_try = 9   action= 0   reward= 1   reward20= 306   \nn_try = 10  action= 1   reward= 100 reward20= 406   \nn_try = 11  action= 0   reward= 1   reward20= 407   \nn_try = 12  action= 0   reward= 1   reward20= 408   \nn_try = 13  action= 0   reward= 1   reward20= 409   \nn_try = 14  action= 1   reward= 100 reward20= 509   \nn_try = 15  action= 1   reward= 100 reward20= 609   \nn_try = 16  action= 0   reward= 1   reward20= 610   \nn_try = 17  action= 1   reward= 100 reward20= 710   \nn_try = 18  action= 0   reward= 1   reward20= 711   \nn_try = 19  action= 0   reward= 1   reward20= 712   \nn_try = 20  action= 1   reward= 100 reward20= 812   \nn_try = 21  action= 1   reward= 100 reward20= 911   \nn_try = 22  action= 0   reward= 1   reward20= 911   \nn_try = 23  action= 0   reward= 1   reward20= 812   \nn_try = 24  action= 0   reward= 1   reward20= 713   \nn_try = 25  action= 0   reward= 1   reward20= 614   \nn_try = 26  action= 0   reward= 1   reward20= 614   \nn_try = 27  action= 0   reward= 1   reward20= 614   \nn_try = 28  action= 0   reward= 1   reward20= 614   \nn_try = 29  action= 0   reward= 1   reward20= 614   \nn_try = 30  action= 0   reward= 1   reward20= 515   \nn_try = 31  action= 1   reward= 100 reward20= 614   \nn_try = 32  action= 1   reward= 100 reward20= 713   \nn_try = 33  action= 0   reward= 1   reward20= 713   \nn_try = 34  action= 1   reward= 100 reward20= 713   \nn_try = 35  action= 1   reward= 100 reward20= 713   \nn_try = 36  action= 0   reward= 1   reward20= 713   \nn_try = 37  action= 1   reward= 100 reward20= 713   \nn_try = 38  action= 0   reward= 1   reward20= 713   \nn_try = 39  action= 1   reward= 100 reward20= 812   \nn_try = 40  action= 0   reward= 1   reward20= 713   \nn_try = 41  action= 1   reward= 100 reward20= 713   \nn_try = 42  action= 0   reward= 1   reward20= 713   \nn_try = 43  action= 1   reward= 100 reward20= 812   \nn_try = 44  action= 1   reward= 100 reward20= 911   \nn_try = 45  action= 1   reward= 100 reward20= 1010  \nn_try = 46  action= 1   reward= 100 reward20= 1109  \nn_try = 47  action= 1   reward= 100 reward20= 1208  \nn_try = 48  action= 0   reward= 1   reward20= 1208  \nn_try = 49  action= 0   reward= 1   reward20= 1208  \nn_try = 50  action= 0   reward= 1   reward20= 1208  \n\n\n\n\n수정2: Env 클래스\n- env 클래스 선언\n\nclass Bandit: \n    def step(self, action):\n        if action == 0:\n            return 1 \n        else: \n            return 100 \n\n\naction_space = gym.spaces.Discrete(2) \nenv = Bandit()\nrewards = []\nfor t in range(50): \n    #action = action_space.sample()\n    action = 1\n    reward = env.step(action)\n    rewards.append(reward)\n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {action}\\t\"\n        f\"reward= {reward}\\t\"\n        f\"reward20= {sum(rewards[-20:])}\\t\"\n    )\n    if np.sum(rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 1   reward= 100 reward20= 100   \nn_try = 2   action= 1   reward= 100 reward20= 200   \nn_try = 3   action= 1   reward= 100 reward20= 300   \nn_try = 4   action= 1   reward= 100 reward20= 400   \nn_try = 5   action= 1   reward= 100 reward20= 500   \nn_try = 6   action= 1   reward= 100 reward20= 600   \nn_try = 7   action= 1   reward= 100 reward20= 700   \nn_try = 8   action= 1   reward= 100 reward20= 800   \nn_try = 9   action= 1   reward= 100 reward20= 900   \nn_try = 10  action= 1   reward= 100 reward20= 1000  \nn_try = 11  action= 1   reward= 100 reward20= 1100  \nn_try = 12  action= 1   reward= 100 reward20= 1200  \nn_try = 13  action= 1   reward= 100 reward20= 1300  \nn_try = 14  action= 1   reward= 100 reward20= 1400  \nn_try = 15  action= 1   reward= 100 reward20= 1500  \nn_try = 16  action= 1   reward= 100 reward20= 1600  \nn_try = 17  action= 1   reward= 100 reward20= 1700  \nn_try = 18  action= 1   reward= 100 reward20= 1800  \nn_try = 19  action= 1   reward= 100 reward20= 1900  \n\n\n\n\n수정3: Agnet 클래스\n- Agent 클래스를 만들자. (액션을 하고, 환경에서 받은 reward를 간직)\n\nclass Agent1:\n    def __init__(self):\n        self.action_space = gym.spaces.Discrete(2) \n        self.action = None \n        self.reward = None \n        self.actions = [] \n        self.rewards = []\n    def act(self):\n        self.action = self.action_space.sample() # 무지한자 \n        #self.action = 1 # 깨달은 자\n    def save_experience(self):\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n\n— 대충 아래와 같은 느낌으로 코드가 돌아가요 —\n시점0: init\n\nenv = Bandit()\nagent = Agent1() \n\n\nagent.action, agent.reward\n\n(None, None)\n\n\n시점1: agent &gt;&gt; env\n\nagent.act()\n\n\nagent.action, agent.reward\n\n(0, None)\n\n\n\nenv.agent_action = agent.action\n\n시점2: agent &lt;&lt; env\n\nagent.reward = env.step(env.agent_action)\n\n\nagent.action, agent.reward, env.agent_action\n\n(0, 1, 0)\n\n\n\nagent.actions,agent.rewards\n\n([], [])\n\n\n\nagent.save_experience()\n\n\nagent.actions,agent.rewards\n\n([0], [1])\n\n\n– 전체코드 –\n\nenv = Bandit() \nagent = Agent1()\nfor t in range(50): \n    ## 1. main 코드 \n    # step1: agent &gt;&gt; env \n    agent.act() \n    env.agent_action = agent.action\n    # step2: agent &lt;&lt; env \n    agent.reward = env.step(env.agent_action)\n    agent.save_experience() \n\n    ## 2. 비본질적 코드 \n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {agent.action}\\t\"\n        f\"reward= {agent.reward}\\t\"\n        f\"reward20= {sum(agent.rewards[-20:])}\\t\"\n    )\n    if np.sum(agent.rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 0   reward= 1   reward20= 1 \nn_try = 2   action= 1   reward= 100 reward20= 101   \nn_try = 3   action= 1   reward= 100 reward20= 201   \nn_try = 4   action= 0   reward= 1   reward20= 202   \nn_try = 5   action= 1   reward= 100 reward20= 302   \nn_try = 6   action= 0   reward= 1   reward20= 303   \nn_try = 7   action= 0   reward= 1   reward20= 304   \nn_try = 8   action= 1   reward= 100 reward20= 404   \nn_try = 9   action= 0   reward= 1   reward20= 405   \nn_try = 10  action= 0   reward= 1   reward20= 406   \nn_try = 11  action= 1   reward= 100 reward20= 506   \nn_try = 12  action= 0   reward= 1   reward20= 507   \nn_try = 13  action= 1   reward= 100 reward20= 607   \nn_try = 14  action= 1   reward= 100 reward20= 707   \nn_try = 15  action= 1   reward= 100 reward20= 807   \nn_try = 16  action= 1   reward= 100 reward20= 907   \nn_try = 17  action= 1   reward= 100 reward20= 1007  \nn_try = 18  action= 1   reward= 100 reward20= 1107  \nn_try = 19  action= 0   reward= 1   reward20= 1108  \nn_try = 20  action= 1   reward= 100 reward20= 1208  \nn_try = 21  action= 0   reward= 1   reward20= 1208  \nn_try = 22  action= 0   reward= 1   reward20= 1109  \nn_try = 23  action= 0   reward= 1   reward20= 1010  \nn_try = 24  action= 1   reward= 100 reward20= 1109  \nn_try = 25  action= 0   reward= 1   reward20= 1010  \nn_try = 26  action= 0   reward= 1   reward20= 1010  \nn_try = 27  action= 1   reward= 100 reward20= 1109  \nn_try = 28  action= 0   reward= 1   reward20= 1010  \nn_try = 29  action= 0   reward= 1   reward20= 1010  \nn_try = 30  action= 1   reward= 100 reward20= 1109  \nn_try = 31  action= 1   reward= 100 reward20= 1109  \nn_try = 32  action= 1   reward= 100 reward20= 1208  \nn_try = 33  action= 1   reward= 100 reward20= 1208  \nn_try = 34  action= 1   reward= 100 reward20= 1208  \nn_try = 35  action= 0   reward= 1   reward20= 1109  \nn_try = 36  action= 0   reward= 1   reward20= 1010  \nn_try = 37  action= 1   reward= 100 reward20= 1010  \nn_try = 38  action= 1   reward= 100 reward20= 1010  \nn_try = 39  action= 0   reward= 1   reward20= 1010  \nn_try = 40  action= 1   reward= 100 reward20= 1010  \nn_try = 41  action= 0   reward= 1   reward20= 1010  \nn_try = 42  action= 0   reward= 1   reward20= 1010  \nn_try = 43  action= 1   reward= 100 reward20= 1109  \nn_try = 44  action= 1   reward= 100 reward20= 1109  \nn_try = 45  action= 0   reward= 1   reward20= 1109  \nn_try = 46  action= 0   reward= 1   reward20= 1109  \nn_try = 47  action= 1   reward= 100 reward20= 1109  \nn_try = 48  action= 0   reward= 1   reward20= 1109  \nn_try = 49  action= 0   reward= 1   reward20= 1109  \nn_try = 50  action= 1   reward= 100 reward20= 1109  \n\n\n\n\n수정4: 학습과정을 포함\n- Game1에 대한 생각:\n\n사실 강화학습은 “환경을 이해 \\(\\to\\) 행동을 결정” 의 과정에서 \\(\\to\\)의 과정을 수식화 한 것이다.\n그런데 지금까지 했던 코드는 환경(env)를 이해하는 순간 에이전트가 최적의 행동(action)2을 직관적으로 결정하였으므로 기계가 스스로 학습을 했다고 볼 수 없다.\n\n2 button1을 누른다- 지금까지의 코드 복습\n\n클래스를 선언하는 부분\n\nEnv 클래스의 선언\nAgent 클래스의 선언\n\n환경과 에이전트를 인스턴스화 (초기화)\nfor loop를 반복하여 게임을 진행\n\n메인코드: (1) agent \\(\\to\\) env (2) agent \\(\\leftarrow\\) env\n비본질적코드: 학습과정을 display, 학습의 종료조건체크\n\n\n- 앞으로 구성할 코드의 형태: 에이전트가 데이터를 보고 스스로 button1을 눌러야 한다는 생각을 했으면 좋겠음.\n\n클래스를 선언하는 부분\n\nEnv 클래스의 선언\nAgent 클래스의 선언 // &lt;—- 학습의 과정이 포함되어야 한다, act함수의 수정, learn함수의 추가\n\n환경과 에이전트를 인스턴스화 (초기화)\nfor loop를 반복하여 게임을 진행\n\n메인코드 (1) agent \\(\\to\\) env (2) agent \\(\\leftarrow\\) env // &lt;—- agent가 데이터를 분석하고 학습하는 과정이 추가\n비본질적코드: 학습과정을 display, 학습의 종료조건체크\n\n\n- 에이전트가 학습을 어떻게 하는가? 아래와 같이 버튼을 누르도록 한다면\n\n버튼0을 누를 확률: \\(\\frac{q_0}{q_0+q_1}\\)\n버튼1을 누를 확률: \\(\\frac{q_1}{q_0+q_1}\\)\n\n시간이 지날수록 버튼1을 주로 누를 것이다.\n- 걱정: \\(t=0\\) 이면 어쩌지? \\(t=1\\)이면 어쩌지?… \\(\\to\\) 해결책: 일정시간동안 랜덤액션을 하면서 데이터를 쌓고 그 뒤에 \\(q_0,q_1\\)을 계산\n- 쌓은 데이터를 바탕으로 환경을 이해하고 action을 뽑는 코드\n\nagent.actions = [0,1,1,0,1,0,0] \nagent.rewards = [1,101,102,1,99,1,1.2] \nactions = np.array(agent.actions)\nrewards = np.array(agent.rewards)\n\n\nq0 = rewards[actions == 0].mean()\nq1 = rewards[actions == 1].mean()\n\n\nagent.q = np.array([q0,q1]) \nagent.q\n\narray([  1.05      , 100.66666667])\n\n\n\nprob = agent.q / agent.q.sum()\nprob \n\narray([0.01032279, 0.98967721])\n\n\n\naction = np.random.choice([0,1], p= agent.q / agent.q.sum())\naction\n\n1\n\n\n- 최종코드정리\n\nclass Bandit: \n    def step(self, action):\n        if action == 0:\n            return 1 \n        else: \n            return 100 \nclass Agent:\n    def __init__(self):\n        self.action_space = gym.spaces.Discrete(2) \n        self.action = None \n        self.reward = None \n        self.actions = [] \n        self.rewards = []\n        self.q = np.array([0,0]) \n        self.n_experience = 0 \n    def act(self):\n        if self.n_experience&lt;30: \n            self.action = self.action_space.sample() \n        else: \n            self.action = np.random.choice([0,1], p= self.q / self.q.sum())\n    def save_experience(self):\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.n_experience += 1 \n    def learn(self):\n        if self.n_experience&lt;30: \n            pass \n        else: \n            actions = np.array(self.actions)\n            rewards = np.array(self.rewards)\n            q0 = rewards[actions == 0].mean()\n            q1 = rewards[actions == 1].mean()\n            self.q = np.array([q0,q1]) \n\n\nenv = Bandit() \nagent = Agent()\nfor t in range(50): \n    ## 1. main 코드 \n    # step1: agent &gt;&gt; env \n    agent.act() \n    env.agent_action = agent.action\n    # step2: agent &lt;&lt; env \n    agent.reward = env.step(env.agent_action)\n    agent.save_experience() \n    # step3: learn \n    agent.learn()\n    ## 2. 비본질적 코드 \n    print(\n        f\"n_try = {t+1}\\t\"\n        f\"action= {agent.action}\\t\"\n        f\"reward= {agent.reward}\\t\"\n        f\"reward20= {sum(agent.rewards[-20:])}\\t\"\n        f\"q = {agent.q}\"\n    )\n    if np.sum(agent.rewards[-20:])&gt;=1900:\n        break \n\nn_try = 1   action= 0   reward= 1   reward20= 1 q = [0 0]\nn_try = 2   action= 0   reward= 1   reward20= 2 q = [0 0]\nn_try = 3   action= 1   reward= 100 reward20= 102   q = [0 0]\nn_try = 4   action= 0   reward= 1   reward20= 103   q = [0 0]\nn_try = 5   action= 0   reward= 1   reward20= 104   q = [0 0]\nn_try = 6   action= 0   reward= 1   reward20= 105   q = [0 0]\nn_try = 7   action= 0   reward= 1   reward20= 106   q = [0 0]\nn_try = 8   action= 0   reward= 1   reward20= 107   q = [0 0]\nn_try = 9   action= 1   reward= 100 reward20= 207   q = [0 0]\nn_try = 10  action= 0   reward= 1   reward20= 208   q = [0 0]\nn_try = 11  action= 1   reward= 100 reward20= 308   q = [0 0]\nn_try = 12  action= 1   reward= 100 reward20= 408   q = [0 0]\nn_try = 13  action= 1   reward= 100 reward20= 508   q = [0 0]\nn_try = 14  action= 0   reward= 1   reward20= 509   q = [0 0]\nn_try = 15  action= 1   reward= 100 reward20= 609   q = [0 0]\nn_try = 16  action= 0   reward= 1   reward20= 610   q = [0 0]\nn_try = 17  action= 0   reward= 1   reward20= 611   q = [0 0]\nn_try = 18  action= 0   reward= 1   reward20= 612   q = [0 0]\nn_try = 19  action= 1   reward= 100 reward20= 712   q = [0 0]\nn_try = 20  action= 1   reward= 100 reward20= 812   q = [0 0]\nn_try = 21  action= 0   reward= 1   reward20= 812   q = [0 0]\nn_try = 22  action= 1   reward= 100 reward20= 911   q = [0 0]\nn_try = 23  action= 0   reward= 1   reward20= 812   q = [0 0]\nn_try = 24  action= 1   reward= 100 reward20= 911   q = [0 0]\nn_try = 25  action= 1   reward= 100 reward20= 1010  q = [0 0]\nn_try = 26  action= 1   reward= 100 reward20= 1109  q = [0 0]\nn_try = 27  action= 1   reward= 100 reward20= 1208  q = [0 0]\nn_try = 28  action= 1   reward= 100 reward20= 1307  q = [0 0]\nn_try = 29  action= 1   reward= 100 reward20= 1307  q = [0 0]\nn_try = 30  action= 1   reward= 100 reward20= 1406  q = [  1. 100.]\nn_try = 31  action= 1   reward= 100 reward20= 1406  q = [  1. 100.]\nn_try = 32  action= 1   reward= 100 reward20= 1406  q = [  1. 100.]\nn_try = 33  action= 1   reward= 100 reward20= 1406  q = [  1. 100.]\nn_try = 34  action= 1   reward= 100 reward20= 1505  q = [  1. 100.]\nn_try = 35  action= 1   reward= 100 reward20= 1505  q = [  1. 100.]\nn_try = 36  action= 1   reward= 100 reward20= 1604  q = [  1. 100.]\nn_try = 37  action= 1   reward= 100 reward20= 1703  q = [  1. 100.]\nn_try = 38  action= 1   reward= 100 reward20= 1802  q = [  1. 100.]\nn_try = 39  action= 1   reward= 100 reward20= 1802  q = [  1. 100.]\nn_try = 40  action= 1   reward= 100 reward20= 1802  q = [  1. 100.]\nn_try = 41  action= 1   reward= 100 reward20= 1901  q = [  1. 100.]"
  },
  {
    "objectID": "posts/A3.html",
    "href": "posts/A3.html",
    "title": "강화학습 (3) – LunarLander",
    "section": "",
    "text": "강의영상\n\n\n\nimports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport torch\nimport collections\nimport IPython\n\n\n\n예비학습\n- collections.deque 의 기능\n\na = collections.deque([1,2,3], maxlen = 5 )\na\n\ndeque([1, 2, 3], maxlen=5)\n\n\n\na.append(4)\na\n\ndeque([1, 2, 3, 4], maxlen=5)\n\n\n\na.append(5)\na\n\ndeque([1, 2, 3, 4, 5], maxlen=5)\n\n\n\na.append(6)\na\n\ndeque([2, 3, 4, 5, 6], maxlen=5)\n\n\n- 단점? numpy array 보다는 list 느낌임 (연산에 특화된건 아님)\n\na + 1\n\nTypeError: can only concatenate deque (not \"int\") to deque\n\n\n- 그렇지만 필요하다면 np.array 화 시킬 수 있음.\n\nnp.array(a) + 1\n\narray([3, 4, 5, 6, 7])\n\n\n- collection.deque 는 리플레이 버퍼를 구현할때 유용한 자료구조이다.\n\n(우리가 했던) 기존방식: 모든 데이터를 저장하며 하나의 경험씩 학습함\n리플레이버퍼: 최근 \\(N\\)개의 데이터를 저장하여 여러경험을 샘플링하여 학습하는 방식\n리플레이버퍼의 장점: 메모리를 아낄 수 있다, 다양한 종류의 경험을 저장하고 무작위로 재사용하여 학습이 안정적으로 된다, “저장 -&gt; 학습 -&gt; 저장” 순으로 반드시 실시간으로 학습할 필요가 없어서 병렬처리에 용이하다, 강화학습에서 연속된 경험은 상관관계가 있을 수 있는데 무작위 샘플로 이러한 상관관계를 제거할 수 있음\n\n\n\nGame3: LunarLander\n- 환경생성\n\nenv = gym.make('LunarLander-v2', render_mode = 'rgb_array') \nenv \n\n&lt;TimeLimit&lt;OrderEnforcing&lt;PassiveEnvChecker&lt;LunarLander&lt;LunarLander-v2&gt;&gt;&gt;&gt;&gt;\n\n\n- state_space\n\nenv.observation_space\n\nBox([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n 1.       ], (8,), float32)\n\n\n- action_space\n\nenv.action_space\n\nDiscrete(4)\n\n\n- env.reset()\n\nstate, _ = env.reset()\nstate \n\narray([-0.004881  ,  1.4137907 , -0.49441272,  0.12757756,  0.00566272,\n        0.11199194,  0.        ,  0.        ], dtype=float32)\n\n\n- env.render()\n\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f9a5477bb10&gt;\n\n\n\n\n\n- env.step\n\nnext_state, reward, terminated, _, _ = env.step(0)\nnext_state, reward, terminated\n\n(array([-0.00976257,  1.4160839 , -0.4937438 ,  0.10189002,  0.01119673,\n         0.11069117,  0.        ,  0.        ], dtype=float32),\n -0.13923681373518093,\n False)\n\n\n- play\n\nenv.reset()\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f9a5349a410&gt;\n\n\n\n\n\n\nfor _ in range(7):\n    env.step(3)\n    env.step(2)\nplt.imshow(env.render())\n\n&lt;matplotlib.image.AxesImage at 0x7f9a534ea410&gt;\n\n\n\n\n\n\n0 : 아무행동도 하지 않음\n1 : 왼쪽\n2 : 위\n3 : 오른쪽\n\n\n\n시각화\n\ndef show(ims,jump=10):\n    ims = ims[::jump]\n    fig = plt.Figure()\n    ax = fig.subplots()\n    def update(i):\n       ax.imshow(ims[i])\n    ani = FuncAnimation(fig,update,frames=len(ims))\n    display(IPython.display.HTML(ani.to_jshtml()))\n\n\ncurrent_state, _ = env.reset()\nims = [] \nfor t in range(500): \n    action = env.action_space.sample()\n    next_state, reward, terminated, _, _ = env.step(action)\n    im = env.render()\n    ims.append(im) \n    current_state = next_state \n    if terminated: break \n\n\nshow(ims) \n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nq_net\n- 원래는 agent.q 에 해당하는 것인데, 이전에서는 agent.q를 (4,4,4) shape의 numpy array 를 사용했는데 여기서는 불가능\n\n4x4 grid: 상태공간의 차원은 2차원이며 가질수 있는 값은 16개, 각 상태공간에서 할수 있는 행동이 4개 -&gt; 총 16*4의 경우의 수에 대한 reward만 조사하면 되었음\nLunarLander: 상태공간의 차원은 8차원이지만 가질수 있는 값의 범위는 무한대 -&gt; 무수히 많은 경우에 대한 reward 값을 조사하는건 현실적으로 불가능\n\n- 데이터를 모아보자.\n\ncurrent_states = collections.deque(maxlen=50) \nactions = collections.deque(maxlen=50) \nnext_states = collections.deque(maxlen=50) \nrewards = collections.deque(maxlen=50) \nterminations = collections.deque(maxlen=50) \n\ncurrent_state, _ = env.reset()\nfor t in range(500): \n    ## step1: agent &gt;&gt; env \n    action = env.action_space.sample()\n    ## step2:agent &lt;&lt; env \n    next_state, reward, terminated, _, _ = env.step(action)\n    current_states.append(current_state)\n    actions.append(action)\n    next_states.append(next_state)\n    rewards.append(reward)\n    terminations.append(terminated) \n    ## step3: learn \n    ## step4: update state     \n    current_state = next_state \n    ## step5: 종료조건체크 \n    if terminated: break \n\n- 이전코드에서 아래에 대응하는 부분을 구현하면 된다.\n## 1. q[x,y,a]를 초기화: q(s)를 넣으면 action에 대한 q값을 알려주는 기능 \nagent.q = np.zeros([4,4,4]) \n\n## 2. q_estimated 를 계산 \nx,y = agent.current_state\nxx,yy = agent.next_state\na = agent.action \nq_estimated = agent.q[x,y,a] \n\n## 3. q_realistic = agent.reward + 0.99 * q_future 를 수행하는 과정 \nif agent.terminated:\n    q_realistic = agent.reward\nelse:\n    q_future = q[xx,yy,:].max()\n    q_realistic = agent.reward + 0.99 * q_future\n\n## 4. q_estimated 를 점점 q_realistic 와 비슷하게 만드는 과정 \ndiff = q_realistic - q_estimated \nagent.q[x,y,a] = q_estimated + 0.05 * diff \n1. agent.q 에 대응하는 과정\n\nq_net = torch.nn.Sequential(\n    torch.nn.Linear(8,128),\n    torch.nn.ReLU(),\n    torch.nn.Linear(128,64),\n    torch.nn.ReLU(),\n    torch.nn.Linear(64,32),\n    torch.nn.ReLU(),\n    torch.nn.Linear(32,4)\n)\n\n\nq_net # &lt;- 8개의 숫자가 입력으로 오면 4개의 숫자를 리턴하는 함수 \n\nSequential(\n  (0): Linear(in_features=8, out_features=128, bias=True)\n  (1): ReLU()\n  (2): Linear(in_features=128, out_features=64, bias=True)\n  (3): ReLU()\n  (4): Linear(in_features=64, out_features=32, bias=True)\n  (5): ReLU()\n  (6): Linear(in_features=32, out_features=4, bias=True)\n)\n\n\n\nq_net(torch.tensor(current_state))\n\ntensor([-0.0863, -0.0824, -0.1490,  0.0031], grad_fn=&lt;AddBackward0&gt;)\n\n\n\nq_net은 8개의 숫자가 입력으로 오면 4개의 숫자가 리턴되는 함수이다.\n해석을 하면 8개의 숫자는 state를 나타내는 숫자로 이해할 수 있고 4개의 숫자는 각 action에 대한 q값으로 해석할 수 있다.\n하지만 이 숫자가 합리적인건 아님 (아무숫자임)\nq_net의 특징: 고정된 함수가 아니고 데이터를 이용하여 점점 더 그럴듯한 숫자를 뱉어내도록 학습할 수 있는 함수이다. (뉴럴네트워크)\n\n1. agent.q 에 대응하는 과정 (배치버전)\n– get batch –\n\nbatch_size = 4 \nidx = np.random.randint(0,50,size=batch_size)\n\ncurrent_states_batch = torch.tensor(np.array(current_states))[idx].float()\nactions_batch = torch.tensor(np.array(actions))[idx].reshape(batch_size,-1) \nrewards_batch = torch.tensor(np.array(rewards))[idx].reshape(batch_size,-1).float()\nnext_states_batch = torch.tensor(np.array(next_states))[idx].float()\nterminations_batch = torch.tensor(np.array(terminations))[idx].reshape(batch_size,-1)\n\n– q_net –\n\ncurrent_states_batch\n\ntensor([[-0.5863,  0.7144, -0.7831, -1.1050,  0.0357, -0.0844,  0.0000,  0.0000],\n        [-0.4805,  1.0306, -0.7311, -0.8693,  0.1304, -0.1544,  0.0000,  0.0000],\n        [-0.6180,  0.6100, -0.7990, -1.1882,  0.0206, -0.0456,  0.0000,  0.0000],\n        [-0.6100,  0.6367, -0.7883, -1.1610,  0.0229, -0.0884,  0.0000,  0.0000]])\n\n\n\nq_net(current_states_batch)\n\ntensor([[-0.0974, -0.0949, -0.0918,  0.0467],\n        [-0.1009, -0.1039, -0.0828,  0.0529],\n        [-0.0953, -0.0925, -0.0947,  0.0437],\n        [-0.0961, -0.0925, -0.0942,  0.0443]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n2. q_estimated\n\nq_net(current_states_batch), actions_batch\n\n(tensor([[-0.0974, -0.0949, -0.0918,  0.0467],\n         [-0.1009, -0.1039, -0.0828,  0.0529],\n         [-0.0953, -0.0925, -0.0947,  0.0437],\n         [-0.0961, -0.0925, -0.0942,  0.0443]], grad_fn=&lt;AddmmBackward0&gt;),\n tensor([[0],\n         [1],\n         [3],\n         [1]]))\n\n\n\nq_net(current_states_batch).gather(1,actions_batch)\n\ntensor([[-0.0974],\n        [-0.1039],\n        [ 0.0437],\n        [-0.0925]], grad_fn=&lt;GatherBackward0&gt;)\n\n\n3. q_realistic = agent.reward + 0.99 * q_future\n– q_future –\n\nq_future = q_net(next_states_batch).max(axis=1)[0].reshape(batch_size,1)\nq_future\n\ntensor([[0.0461],\n        [0.0538],\n        [0.0421],\n        [0.0437]], grad_fn=&lt;ReshapeAliasBackward0&gt;)\n\n\n\nq_realistic = rewards_batch + 0.99 * q_future * (~terminations_batch)\n\n4. q_estimated 를 점점 q_realistic 와 비슷하게 만드는 과정\n## 여기는.. 딥러닝과 파이토치를 좀 알아야.. 모른다면 일단 패스해야합니다.. \noptimizer = torch.optim.Adam(q_net.parameters(),lr=0.0001) \nfor _ in range(2000):\n    ~~~\n    ~~~\n    q_estimated = ~~~ \n    q_realistic = ~~~ \n    loss = torch.nn.functional.mse_loss(q_estimated,q_realistic)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n\npolicy\n\neps = 0.5 \nif np.random.rand() &lt; eps:\n    action = env.action_space.sample() \nelse:\n    action = q_net(torch.tensor(current_state)).argmax().item()\n\n\naction\n\n3\n\n\n\n\nAgent 클래스 + run\n\nclass Agent():\n    def __init__(self,env):\n        self.eps = 0\n        self.n_experiences = 0\n        self.n_episode = 0\n        self.score = 0\n        self.scores = []\n        self.playtimes = []\n        self.batch_size = 64\n        self.buffer_size = 5000 \n        self.action_space = env.action_space\n        #self.state_space = env.observation_space\n\n        # Q-Network\n        self.q_net = torch.nn.Sequential(\n            torch.nn.Linear(8,128), \n            torch.nn.ReLU(),\n            torch.nn.Linear(128,64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32,4)\n        ) \n        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=0.0001)\n\n        # ReplayBuffer\n        self.current_states = collections.deque(maxlen=self.buffer_size)\n        self.actions = collections.deque(maxlen=self.buffer_size)\n        self.rewards = collections.deque(maxlen=self.buffer_size)\n        self.next_states = collections.deque(maxlen=self.buffer_size)\n        self.terminations = collections.deque(maxlen=self.buffer_size)\n       \n    def save_experience(self):\n        \"\"\"Add a new experience to memory.\"\"\"\n        self.current_states.append(self.current_state)\n        self.actions.append(self.action)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated) \n        self.n_experiences = self.n_experiences+1\n        self.score += self.reward\n    \n    def act(self):\n        if np.random.rand() &lt; self.eps:\n            self.action = self.action_space.sample()\n        else:\n            self.action = self.q_net(torch.tensor(self.current_state)).argmax().item()\n            \n    def get_batch(self):\n        idx = np.random.randint(0,self.buffer_size,size=self.batch_size) \n        self.current_states_batch = torch.tensor(np.array(self.current_states))[idx].float()\n        self.actions_batch = torch.tensor(np.array(self.actions))[idx].reshape(self.batch_size,1)\n        self.rewards_batch = torch.tensor(np.array(self.rewards))[idx].reshape(self.batch_size,-1).float()\n        self.next_states_batch = torch.tensor(np.array(self.next_states))[idx].float()\n        self.terminations_batch = torch.tensor(np.array(self.terminations))[idx].reshape(self.batch_size,-1) \n    \n    def learn(self):\n        if self.n_experiences &lt; self.buffer_size:\n            pass\n        else: \n            self.get_batch()\n            q_estimated = self.q_net(self.current_states_batch).gather(1, self.actions_batch)\n            q_future = self.q_net(self.next_states_batch).detach().max(1)[0].reshape(self.batch_size,1)\n            q_realistic = self.rewards_batch + 0.99 * q_future * (~self.terminations_batch)\n\n            loss = torch.nn.functional.mse_loss(q_estimated, q_realistic)\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n\nenv = gym.make('LunarLander-v2',render_mode='rgb_array')\nagent = Agent(env)\nagent.eps = 1.0 \nfor _ in range(2000):\n    ### 1. 본질적인 코드\n    agent.current_state, _  = env.reset() \n    agent.terminated = False\n    agent.score = 0 \n    for t in range(500):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated, _,_ = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episode = agent.n_episode + 1 \n    agent.eps = agent.eps*0.995\n    ## 2. 비본질적 코드\n    if (agent.n_episode % 10) == 0:\n        print(\n            f'Episode {agent.n_episode}\\t'\n            f'Score: {np.mean(agent.scores[-100:]) : .2f}\\t'\n            f'Playtime: {np.mean(agent.playtimes[-100:]) : .2f}\\t'\n            f'n_eps: {agent.eps}\\t'\n            f'n_experiences: {agent.n_experiences}\\t'\n        )\n    if np.mean(agent.scores[-100:])&gt;=200.0:\n        break\n\nEpisode 10  Score: -213.18  Playtime:  92.70    n_eps: 0.9511101304657719   n_experiences: 927  \nEpisode 20  Score: -204.70  Playtime:  99.50    n_eps: 0.9046104802746175   n_experiences: 1990 \nEpisode 30  Score: -211.72  Playtime:  104.50   n_eps: 0.8603841919146962   n_experiences: 3135 \nEpisode 40  Score: -226.75  Playtime:  105.53   n_eps: 0.8183201210226743   n_experiences: 4221 \nEpisode 50  Score: -208.68  Playtime:  106.34   n_eps: 0.778312557068642    n_experiences: 5317 \nEpisode 60  Score: -197.43  Playtime:  108.47   n_eps: 0.7402609576967045   n_experiences: 6508 \nEpisode 70  Score: -208.33  Playtime:  115.60   n_eps: 0.7040696960536299   n_experiences: 8092 \nEpisode 80  Score: -212.00  Playtime:  117.41   n_eps: 0.6696478204705644   n_experiences: 9393 \nEpisode 90  Score: -208.73  Playtime:  118.74   n_eps: 0.6369088258938781   n_experiences: 10687    \nEpisode 100 Score: -206.49  Playtime:  119.74   n_eps: 0.6057704364907278   n_experiences: 11974    \nEpisode 110 Score: -196.32  Playtime:  123.45   n_eps: 0.5761543988830038   n_experiences: 13272    \nEpisode 120 Score: -184.32  Playtime:  129.69   n_eps: 0.547986285490042    n_experiences: 14959    \nEpisode 130 Score: -172.21  Playtime:  130.03   n_eps: 0.5211953074858876   n_experiences: 16138    \nEpisode 140 Score: -152.05  Playtime:  142.30   n_eps: 0.49571413690105054  n_experiences: 18451    \nEpisode 150 Score: -143.29  Playtime:  146.41   n_eps: 0.47147873742168567  n_experiences: 19958    \nEpisode 160 Score: -132.48  Playtime:  154.47   n_eps: 0.4484282034609769   n_experiences: 21955    \nEpisode 170 Score: -106.66  Playtime:  163.24   n_eps: 0.42650460709830135  n_experiences: 24416    \nEpisode 180 Score: -85.85   Playtime:  180.09   n_eps: 0.40565285250151817  n_experiences: 27402    \nEpisode 190 Score: -73.39   Playtime:  201.63   n_eps: 0.3858205374665315   n_experiences: 30850    \nEpisode 200 Score: -49.93   Playtime:  230.35   n_eps: 0.3669578217261671   n_experiences: 35009    \nEpisode 210 Score: -40.46   Playtime:  263.12   n_eps: 0.34901730169741024  n_experiences: 39584    \nEpisode 220 Score: -31.11   Playtime:  280.75   n_eps: 0.33195389135223546  n_experiences: 43034    \nEpisode 230 Score: -15.80   Playtime:  314.08   n_eps: 0.3157247089126454   n_experiences: 47546    \nEpisode 240 Score: -5.43    Playtime:  333.12   n_eps: 0.30028896908517405  n_experiences: 51763    \nEpisode 250 Score:  4.33    Playtime:  363.03   n_eps: 0.285607880564032    n_experiences: 56261    \nEpisode 260 Score:  10.80   Playtime:  391.83   n_eps: 0.27164454854530906  n_experiences: 61138    \nEpisode 270 Score:  14.88   Playtime:  413.84   n_eps: 0.2583638820072446   n_experiences: 65800    \nEpisode 280 Score:  21.58   Playtime:  432.86   n_eps: 0.2457325055235537   n_experiences: 70688    \nEpisode 290 Score:  31.61   Playtime:  443.43   n_eps: 0.23371867538818816  n_experiences: 75193    \nEpisode 300 Score:  29.04   Playtime:  439.61   n_eps: 0.22229219984074702  n_experiences: 78970    \nEpisode 310 Score:  37.79   Playtime:  443.86   n_eps: 0.21142436319205632  n_experiences: 83970    \nEpisode 320 Score:  43.76   Playtime:  456.21   n_eps: 0.2010878536592394   n_experiences: 88655    \nEpisode 330 Score:  43.98   Playtime:  461.09   n_eps: 0.1912566947289212   n_experiences: 93655    \nEpisode 340 Score:  45.86   Playtime:  468.92   n_eps: 0.18190617987607657  n_experiences: 98655    \nEpisode 350 Score:  50.36   Playtime:  473.94   n_eps: 0.1730128104744653   n_experiences: 103655   \nEpisode 360 Score:  51.94   Playtime:  467.59   n_eps: 0.16455423674261854  n_experiences: 107897   \nEpisode 370 Score:  51.95   Playtime:  467.90   n_eps: 0.15650920157696743  n_experiences: 112590   \nEpisode 380 Score:  55.56   Playtime:  469.02   n_eps: 0.14885748713096328  n_experiences: 117590   \nEpisode 390 Score:  62.16   Playtime:  473.97   n_eps: 0.14157986400593744  n_experiences: 122590   \nEpisode 400 Score:  71.21   Playtime:  485.75   n_eps: 0.1346580429260134   n_experiences: 127545   \nEpisode 410 Score:  82.20   Playtime:  479.03   n_eps: 0.12807462877562611  n_experiences: 131873   \nEpisode 420 Score:  96.33   Playtime:  473.30   n_eps: 0.12181307688414106  n_experiences: 135985   \nEpisode 430 Score:  115.43  Playtime:  466.74   n_eps: 0.11585765144771248  n_experiences: 140329   \nEpisode 440 Score:  118.52  Playtime:  463.05   n_eps: 0.11019338598389174  n_experiences: 144960   \nEpisode 450 Score:  117.20  Playtime:  463.05   n_eps: 0.10480604571960442  n_experiences: 149960   \nEpisode 460 Score:  135.12  Playtime:  460.61   n_eps: 0.0996820918179746   n_experiences: 153958   \nEpisode 470 Score:  156.37  Playtime:  447.12   n_eps: 0.09480864735409487  n_experiences: 157302   \nEpisode 480 Score:  175.40  Playtime:  433.66   n_eps: 0.09017346495423652  n_experiences: 160956   \nEpisode 490 Score:  191.98  Playtime:  417.28   n_eps: 0.08576489601717459  n_experiences: 164318   \n\n\n- 시각화를 위한코드\n\nagent2 = Agent(env) \nagent2.q_net = agent.q_net\n\nagent2.current_state, _ = env.reset()\nagent2.terminated = False \nims = [] \nims.append(env.render())\nfor t in range(500):\n    agent2.act() \n    agent2.next_state, agent2.reward, agent2.terminated, _, _  = env.step(agent2.action)\n    im = env.render()\n    ims.append(im)\n    agent2.current_state = agent2.next_state\n    if agent2.terminated: break \n\n\nshow(ims)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  },
  {
    "objectID": "posts/A2.html",
    "href": "posts/A2.html",
    "title": "강화학습 (2) – 4x4 grid",
    "section": "",
    "text": "강의영상\n\n\n\nGame2: 4x4 grid\n- 문제설명: 4x4 그리드월드에서 상하좌우로 움직이는 에이전트가 목표점에 도달하도록 학습하는 방법\n\n\nimports\n\nimport gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport IPython\n\n\n\n예비학습: 시각화\n\ndef show(states):\n    fig = plt.Figure()\n    ax = fig.subplots()\n    ax.matshow(np.zeros([4,4]), cmap='bwr',alpha=0.0)\n    sc = ax.scatter(0, 0, color='red', s=500)  \n    ax.text(0, 0, 'start', ha='center', va='center')\n    ax.text(3, 3, 'end', ha='center', va='center')\n    # Adding grid lines to the plot\n    ax.set_xticks(np.arange(-.5, 4, 1), minor=True)\n    ax.set_yticks(np.arange(-.5, 4, 1), minor=True)\n    ax.grid(which='minor', color='black', linestyle='-', linewidth=2)\n    def update(t):\n        sc.set_offsets(states[t])\n    ani = FuncAnimation(fig,update,frames=len(states))\n    display(IPython.display.HTML(ani.to_jshtml()))\n\n\nshow([[0,0],[0,1],[1,1],[1,2],[1,3],[1,2],[1,3],[1,2],[1,3],[1,2],[1,3]])\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nEnv 클래스 구현\n- GridWorld: 강화학습에서 많이 예시로 사용되는 기본적인 시뮬레이션 환경\n\nState: 각 격자 셀이 하나의 상태이며, 에이전트는 이러한 상태 중 하나에 있을 수 있음.\nAction: 에이전트는 현재상태에서 다음상태로 이동하기 위해 상,하,좌,우 중 하나의 행동을 취할 수 있음.\nReward: 에이전트가 현재상태에서 특정 action을 하면 얻어지는 보상\nTerminated: 하나의 에피소드가 종료되었음을 나타내는 상태\n\n\naction = 3\ncurrent_state = np.array([1,1])\n\n\nnext_state = current_state + action_to_direction[action]\nnext_state\n\nNameError: name 'action_to_direction' is not defined\n\n\n\nclass GridWorld:\n    def __init__(self):\n        self.reset()\n        self.state_space = gym.spaces.MultiDiscrete([4,4])\n        self.action_space = gym.spaces.Discrete(4) \n        self._action_to_direction = { \n            0 : np.array([1, 0]), # x+ \n            1 : np.array([0, 1]), # y+ \n            2 : np.array([-1 ,0]), # x- \n            3 : np.array([0, -1]) # y- \n        }\n    def reset(self):\n        self.agent_action = None \n        self.agent_state = np.array([0,0])        \n        return self.agent_state \n    def step(self,action):\n        direction = self._action_to_direction[action]\n        self.agent_state = self.agent_state + direction\n        if self.agent_state not in env.state_space: # 4x4 그리드 밖에 있는 경우\n            reward = -10 \n            terminated = True\n            self.agent_state = self.agent_state -1/2 * direction\n        elif np.array_equal(env.agent_state, np.array([3,3])): # 목표지점에 도달할 경우 \n            reward = 100 \n            terminated = True\n        else: \n            reward = -1 \n            terminated = False         \n        return self.agent_state, reward, terminated\n\n\nenv = GridWorld()\n\n\nstates = [] \nstate = env.reset()\nstates.append(state) \nfor t in range(50):\n    action = env.action_space.sample() \n    state,reward,terminated = env.step(action)\n    states.append(state) \n    if terminated: break \n\n\nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\nAgent1 클래스 구현 + Run\n- 우리가 구현하고 싶은 기능\n\n.act(): 액션을 결정 –&gt; 여기서는 그냥 랜덤액션\n.save_experience(): 데이터를 저장 –&gt; 여기에 일단 초점을 맞추자\n.learn(): 데이터로에서 학습 –&gt; 패스\n\n- 첫번째 시도\n\nclass Agent1:\n    def __init__(self,env):\n        self.action_space = env.action_space\n        self.state_spcae = env.state_space \n        self.n_experiences = 0 \n        self.n_episodes = 0 \n        self.score = 0 \n        \n        # episode-wise info \n        self.scores = [] \n        self.playtimes = []\n\n        # time-wise info\n        self.current_state = None \n        self.action = None \n        self.reward = None \n        self.next_state = None         \n        self.terminated = None \n\n        # replay_buffer \n        self.actions = []\n        self.current_states = [] \n        self.rewards = []\n        self.next_states = [] \n        self.terminations = [] \n\n    def act(self):\n        self.action = self.action_space.sample() \n\n    def save_experience(self):\n        self.actions.append(self.action) \n        self.current_states.append(self.current_state)\n        self.rewards.append(self.reward)\n        self.next_states.append(self.next_state)\n        self.terminations.append(self.terminated) \n        self.n_experiences += 1 \n        self.score = self.score + self.reward \n        \n    def learn(self):\n        pass \n\n\nenv = GridWorld() \nagent = Agent1(env) \nfor _ in range(20):\n    ## 본질적인 코드 \n    agent.current_state = env.reset()\n    agent.terminated = False \n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        # agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episodes = agent.n_episodes + 1 \n    ## 덜 본질적인 코드 \n    print(\n        f\"Epsiode: {agent.n_episodes} \\t\"\n        f\"Score: {agent.scores[-1]} \\t\"\n        f\"Playtime: {agent.playtimes[-1]}\"\n    )   \n\nEpsiode: 1  Score: -21  Playtime: 12\nEpsiode: 2  Score: -10  Playtime: 1\nEpsiode: 3  Score: -11  Playtime: 2\nEpsiode: 4  Score: -10  Playtime: 1\nEpsiode: 5  Score: -10  Playtime: 1\nEpsiode: 6  Score: -11  Playtime: 2\nEpsiode: 7  Score: -18  Playtime: 9\nEpsiode: 8  Score: 93   Playtime: 8\nEpsiode: 9  Score: -13  Playtime: 4\nEpsiode: 10     Score: -13  Playtime: 4\nEpsiode: 11     Score: -18  Playtime: 9\nEpsiode: 12     Score: -10  Playtime: 1\nEpsiode: 13     Score: -10  Playtime: 1\nEpsiode: 14     Score: -10  Playtime: 1\nEpsiode: 15     Score: -10  Playtime: 1\nEpsiode: 16     Score: -16  Playtime: 7\nEpsiode: 17     Score: -10  Playtime: 1\nEpsiode: 18     Score: -24  Playtime: 15\nEpsiode: 19     Score: -13  Playtime: 4\nEpsiode: 20     Score: -10  Playtime: 1\n\n\n\nsum(agent.playtimes[:7])\n\n28\n\n\n\nsum(agent.playtimes[:8])\n\n36\n\n\n\nstates = [np.array([0,0])] + agent.next_states[28:36]\nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n우연히 잘맞춘 케이스\n\n\n\n환경의 이해 (1차원적 이해)\n- 무작위로 10000판을 진행해보자.\n\nenv = GridWorld() \nagent = Agent1(env) \nfor _ in range(10000):\n    ## 본질적인 코드 \n    agent.current_state = env.reset()\n    agent.terminated = False \n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        # agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episodes = agent.n_episodes + 1 \n\n\nagent.n_experiences\n\n33249\n\n\n- 데이터관찰\n\nagent.current_states[0], agent.actions[0], agent.rewards[0], agent.next_states[0]\n\n(array([0, 0]), 0, -1, array([1, 0]))\n\n\n\nagent.current_states[1], agent.actions[1], agent.rewards[1], agent.next_states[1]\n\n(array([1, 0]), 1, -1, array([1, 1]))\n\n\n\nagent.current_states[2], agent.actions[2], agent.rewards[2], agent.next_states[2]\n\n(array([1, 1]), 1, -1, array([1, 2]))\n\n\n\nagent.current_states[3], agent.actions[3], agent.rewards[3], agent.next_states[3]\n\n(array([1, 2]), 2, -1, array([0, 2]))\n\n\n\nagent.current_states[4], agent.actions[4], agent.rewards[4], agent.next_states[4]\n\n(array([0, 2]), 1, -1, array([0, 3]))\n\n\n- 환경을 이해하기 위한 기록 (1)\n\nq = np.zeros([4,4,4])\ncount = np.zeros([4,4,4])\nfor i in range(agent.n_experiences):\n    x,y = agent.current_states[i] \n    a = agent.actions[i] \n    q[x,y,a] = q[x,y,a] + agent.rewards[i] \n    count[x,y,a] = count[x,y,a] + 1 \n\n\ncount[count == 0] = 0.01 \nq = q/count\n\n\nq[:,:,3]\n\narray([[-10.,  -1.,  -1.,  -1.],\n       [-10.,  -1.,  -1.,  -1.],\n       [-10.,  -1.,  -1.,  -1.],\n       [-10.,  -1.,  -1.,   0.]])\n\n\n\nfor a in range(4):\n    print(\n        f\"action = {a}\\n\" \n        f\"action-value function = \\n {q[:,:,a]}\\n\" \n)\n\naction = 0\naction-value function = \n [[ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1. 100.]\n [-10. -10. -10.   0.]]\n\naction = 1\naction-value function = \n [[ -1.  -1.  -1. -10.]\n [ -1.  -1.  -1. -10.]\n [ -1.  -1.  -1. -10.]\n [ -1.  -1. 100.   0.]]\n\naction = 2\naction-value function = \n [[-10. -10. -10. -10.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.  -1.]\n [ -1.  -1.  -1.   0.]]\n\naction = 3\naction-value function = \n [[-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.  -1.]\n [-10.  -1.  -1.   0.]]\n\n\n\n- 환경을 이해하기 위한 기록 (2)\n\nq = np.zeros([4,4,4])\nfor i in range(agent.n_experiences):\n    x,y = agent.current_states[i]\n    a = agent.actions[i]\n    q_estimated = q[x,y,a] # 우리가 환경을 이해하고 있는 값, 우리가 풀어낸 답 \n    q_realistic = agent.rewards[i] # 실제 답 \n    diff = q_realistic - q_estimated # 실제답과 풀이한값의 차이 = 오차피드백값 \n    q[x,y,a] = q_estimated + 0.05 * diff ## 새로운답 = 원래답 + 오차피드백값 \n\n\nfor a in range(4):\n    print(\n        f\"action = {a}\\n\" \n        f\"action-value function = \\n {q[:,:,a]}\\n\" \n)\n\naction = 0\naction-value function = \n [[-1.         -1.         -1.         -0.99866234]\n [-1.         -1.         -1.         -0.99851783]\n [-0.99999999 -1.         -0.99999593 98.43103943]\n [-9.97394217 -9.99697776 -9.93439857  0.        ]]\n\naction = 1\naction-value function = \n [[-1.         -1.         -1.         -9.98591939]\n [-1.         -1.         -0.99999996 -9.99588862]\n [-1.         -0.99999999 -0.99999593 -9.92731143]\n [-0.99915694 -0.99971289 98.50948746  0.        ]]\n\naction = 2\naction-value function = \n [[-10.         -10.          -9.99999999  -9.99065864]\n [ -1.          -1.          -0.99999999  -0.99923914]\n [ -1.          -1.          -0.99999321  -0.9884667 ]\n [ -0.99946866  -0.99981905  -0.99465672   0.        ]]\n\naction = 3\naction-value function = \n [[-10.          -1.          -1.          -0.99919909]\n [-10.          -1.          -1.          -0.99866234]\n [ -9.99999999  -1.          -0.99999285  -0.99541881]\n [ -9.99347658  -0.99987363  -0.99776587   0.        ]]\n\n\n\n\n\n환경의 깊은 이해 (좀 더 고차원적인 이해)\n- action=1 일때 각 state의 가치 (=기대보상)\n\nq[:,:,1]\n\narray([[-1.        , -1.        , -1.        , -9.98591939],\n       [-1.        , -1.        , -0.99999996, -9.99588862],\n       [-1.        , -0.99999999, -0.99999593, -9.92731143],\n       [-0.99915694, -0.99971289, 98.50948746,  0.        ]])\n\n\n- 분석1\n\nq[3,2,1]\n\n98.50948746175251\n\n\n\n상태 (3,2)에서 행동 1을 하게되면 100의 보상을 얻으므로 기대보상값은 100근처 –&gt; 합리적임\n\n- 분석2\n\nq[3,1,1]\n\n-0.9997128867462345\n\n\n\n상태 (3,1)에서 행동 1을 하게되면 -1 의 보상을 얻으므로 기대보상값은 -1 근처 –&gt; 합리적일까??\n\n- 비판: 분석2는 합리적인것 처럼 보이지만 data를 분석한 뒤에는 그다지 합리적이지 못함\n- 상황상상\n\n빈 종이를 줌\n빈 종이에는 0 또는 1을 쓸 수 있음 (action = 0 혹은 1)\n0을 쓸때와 1을 쓸때 보상이 다름\n무수히 많은 데이터를 분석해보니, 0을 쓰면 0원을 주고 1을 쓰면 10만원을 보상을 준다는 것을 “알게 되었음”\n이때 빈 종이의 가치는 5만원인가? 10만원인가? –&gt; 10만원아니야?\n\n- 직관: 생각해보니 현재 \\(s=(3,1)\\) \\(a=1\\)에서 추정된(esitated) 값은 q[3,1,1]= -0.9997128867462345 이지만1, 현실적으로는 “실제보상(-1)과 잠재적보상(100)”을 동시에 고려해야 하는게 합리적임\n1 즉 next_state가 가지는 잠재적값어치는 고려되어있지 않음\nq_estimated = q[3,1,1]\nq_estimated\n\n-0.9997128867462345\n\n\n\nq_realistic = (-1) + 0.99 * 100 \nq_realistic\n\n98.0\n\n\n\n여기에서 0.99는 “미래에 받을 보상이 현재에 비해 얼마나 중요한지를 결정하는 가중치” 이다.\n1에 가까울수록 미래에 받을 보상을 매우 중시한다는 의미 (즉 빈종이= 십만원 으로 생각한다는 의미)\n\n- 즉 \\(q(s,a)\\)는 모든 \\(s\\), \\(a\\)에 대하여\n\\[q(s,a) \\approx \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a)\\]\n가 성립한다면 \\(q(s,a)\\)는 타당하게 추정된 것이라 볼 수 있다. 물론 수식을 좀 더 엄밀하게 쓰면 아래와 같다.\n\\[q(s,a) \\approx \\begin{cases} \\text{reward}(s,a) & \\text{terminated} \\\\  \\text{reward}(s,a) + 0.99 \\times \\max_{a}q(s',a) & \\text{not terminated}\\end{cases}\\]\n\nq = np.zeros([4,4,4])\nfor i in range(agent.n_experiences):\n    x,y = agent.current_states[i]\n    xx,yy = agent.next_states[i]\n    a = agent.actions[i]\n    q_estimated = q[x,y,a] \n    if agent.terminations[i]:\n        q_realistic = agent.rewards[i]\n    else:\n        q_future = q[xx,yy,:].max()\n        q_realistic = agent.rewards[i] + 0.99 * q_future\n    diff = q_realistic - q_estimated \n    q[x,y,a] = q_estimated + 0.05 * diff \n\n\nfor a in range(4):\n    print(\n        f\"action = {a}\\n\" \n        f\"action-value function = \\n {q[:,:,a]}\\n\" \n)\n\naction = 0\naction-value function = \n [[87.02554961 88.94759484 90.75390245 88.54847007]\n [88.4709728  91.06852327 93.18709107 94.21998722]\n [84.98258538 91.44091272 95.48024593 98.43103943]\n [-9.97394217 -9.99697776 -9.93439857  0.        ]]\n\naction = 1\naction-value function = \n [[87.01670813 88.59888111 85.52951661 -9.98591939]\n [88.98190464 91.03081993 91.50379877 -9.99588862]\n [90.76721433 93.24316728 95.65715857 -9.92731143]\n [89.20612688 94.47295823 98.50948746  0.        ]]\n\naction = 2\naction-value function = \n [[-10.         -10.          -9.99999999  -9.99065864]\n [ 84.96179325  86.84873675  88.0518007   80.10750712]\n [ 86.40784936  88.69218405  89.83203868  83.06339754]\n [ 86.40852121  89.09508079  89.87262647   0.        ]]\n\naction = 3\naction-value function = \n [[-10.          84.96186287  86.49128928  84.57992176]\n [-10.          86.73523202  88.56505447  86.7154156 ]\n [ -9.99999999  88.3058275   90.27264766  87.96618484]\n [ -9.99347658  80.88548565  86.63274331   0.        ]]\n\n\n\n\n\n행동 전략 수립\n- 상태 (0,0)에 있다고 가정해보자.\n\nq[0,0,:]\n\narray([ 87.02554961,  87.01670813, -10.        , -10.        ])\n\n\n\n행동 0 혹은 행동 1을 하는게 유리하다. // 행동 2,3을 하면 망한다.\n\n- 상태 (2,3)에 있다고 가정해보자.\n\nq[2,3,:]\n\narray([98.43103943, -9.92731143, 83.06339754, 87.96618484])\n\n\n\n행동 0을 하는게 유리함.\n\n- 상태 (3,2)에 있다고 가정해보자.\n\nq[3,2,:]\n\narray([-9.93439857, 98.50948746, 89.87262647, 86.63274331])\n\n\n\n행동1을 하는게 유리함\n\n- 각 상태에서 최적은 action은 아래와 같다.\n\nq[0,0,:].argmax()\n\n0\n\n\n\nq[2,3,:].argmax()\n\n0\n\n\n\nq[3,2,:].argmax()\n\n1\n\n\n- 전략(=정책)을 정리해보자.\n\npolicy = np.array(['?????']*16).reshape(4,4)\npolicy\n\narray([['?????', '?????', '?????', '?????'],\n       ['?????', '?????', '?????', '?????'],\n       ['?????', '?????', '?????', '?????'],\n       ['?????', '?????', '?????', '?????']], dtype='&lt;U5')\n\n\n\ndirections = {0:'down', 1: 'right', 2:'up', 3:'left'} \n\n\nfor x in range(4):\n    for y in range(4):\n        policy[x,y] = directions[q[x,y,:].argmax()]\npolicy\n\narray([['down', 'down', 'down', 'down'],\n       ['right', 'down', 'down', 'down'],\n       ['right', 'right', 'right', 'down'],\n       ['right', 'right', 'right', 'down']], dtype='&lt;U5')\n\n\n\nq.max(axis=-1)\n\narray([[87.02554961, 88.94759484, 90.75390245, 88.54847007],\n       [88.98190464, 91.06852327, 93.18709107, 94.21998722],\n       [90.76721433, 93.24316728, 95.65715857, 98.43103943],\n       [89.20612688, 94.47295823, 98.50948746,  0.        ]])\n\n\n\n\nAgent2 클래스 구현 + Run\n\nclass Agent2(Agent1):\n    def __init__(self,env):\n        super().__init__(env)\n        self.q = np.zeros([4,4,4]) \n    def learn(self):\n        x,y = self.current_state\n        xx,yy = self.next_state\n        a = self.action \n        q_estimated = self.q[x,y,a] \n        if self.terminated:\n            q_realistic = self.reward\n        else:\n            q_future = q[xx,yy,:].max()\n            q_realistic = self.reward + 0.99 * q_future\n        diff = q_realistic - q_estimated \n        self.q[x,y,a] = q_estimated + 0.05 * diff \n    def act(self):\n        if self.n_experiences &lt; 3000: \n            self.action = self.action_space.sample() \n        else:\n            x,y = self.current_state \n            self.action = self.q[x,y,:].argmax()\n\n\nenv = GridWorld() \nagent = Agent2(env) \nfor _ in range(2000):\n    ## 본질적인 코드 \n    agent.current_state = env.reset()\n    agent.terminated = False \n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episodes = agent.n_episodes + 1 \n    ## 덜 본질적인 코드 \n    if (agent.n_episodes % 100) ==0:\n        print(\n            f\"Epsiode: {agent.n_episodes} \\t\"\n            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n            f\"Playtime: {np.mean(agent.playtimes[-100:])}\"\n        )   \n\nEpsiode: 100    Score: -10.36   Playtime: 3.56\nEpsiode: 200    Score: -10.9    Playtime: 3.0\nEpsiode: 300    Score: -11.02   Playtime: 3.12\nEpsiode: 400    Score: -6.64    Playtime: 4.24\nEpsiode: 500    Score: -11.08   Playtime: 3.18\nEpsiode: 600    Score: -10.53   Playtime: 3.73\nEpsiode: 700    Score: -9.96    Playtime: 3.16\nEpsiode: 800    Score: -8.6     Playtime: 2.9\nEpsiode: 900    Score: -13.6    Playtime: 7.61\nEpsiode: 1000   Score: -50.0    Playtime: 50.0\nEpsiode: 1100   Score: -50.0    Playtime: 50.0\nEpsiode: 1200   Score: -50.0    Playtime: 50.0\nEpsiode: 1300   Score: -50.0    Playtime: 50.0\nEpsiode: 1400   Score: -50.0    Playtime: 50.0\nEpsiode: 1500   Score: -50.0    Playtime: 50.0\nEpsiode: 1600   Score: -50.0    Playtime: 50.0\nEpsiode: 1700   Score: -50.0    Playtime: 50.0\nEpsiode: 1800   Score: -50.0    Playtime: 50.0\nEpsiode: 1900   Score: -50.0    Playtime: 50.0\nEpsiode: 2000   Score: -50.0    Playtime: 50.0\n\n\n\nstates = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\nagent.q.max(-1).T\n\narray([[87.0920856 , 89.15783804, 76.99095205, 51.80408762],\n       [88.62997645, 91.25522016, 82.83298477, 55.040804  ],\n       [91.25522016, 88.84636342, 73.50910388, 22.62190625],\n       [40.8373347 , 51.7638053 , 45.96399123,  0.        ]])\n\n\n\n\nAgnet3 클래스 구현 + Run\n\nclass Agent3(Agent2):\n    def __init__(self,env):\n        super().__init__(env)\n        self.eps = 0 \n    def act(self):\n        if np.random.rand() &lt; self.eps:\n            self.action = self.action_space.sample() \n        else:\n            x,y = self.current_state \n            self.action = self.q[x,y,:].argmax()\n\n\nenv = GridWorld() \nagent = Agent3(env) \nagent.eps = 1\nfor _ in range(5000):\n    ## 본질적인 코드 \n    agent.current_state = env.reset()\n    agent.terminated = False \n    agent.score = 0 \n    for t in range(50):\n        # step1: agent &gt;&gt; env \n        agent.act() \n        env.agent_action = agent.action  \n        # step2: agent &lt;&lt; env \n        agent.next_state, agent.reward, agent.terminated = env.step(env.agent_action)\n        agent.save_experience() \n        # step3: learn \n        agent.learn()\n        # step4: state update \n        agent.current_state = agent.next_state \n        # step5: \n        if agent.terminated: break \n    agent.scores.append(agent.score) \n    agent.playtimes.append(t+1)\n    agent.n_episodes = agent.n_episodes + 1\n    agent.eps = agent.eps * 0.999\n    ## 덜 본질적인 코드 \n    if (agent.n_episodes % 200) ==0:\n        print(\n            f\"Epsiode: {agent.n_episodes} \\t\"\n            f\"Score: {np.mean(agent.scores[-100:])} \\t\"\n            f\"Playtime: {np.mean(agent.playtimes[-100:])}\\t\"\n            f\"Epsilon: {agent.eps : .2f}\"\n        )   \n\nEpsiode: 200    Score: -8.49    Playtime: 3.89  Epsilon:  0.82\nEpsiode: 400    Score: -9.83    Playtime: 4.13  Epsilon:  0.67\nEpsiode: 600    Score: -10.72   Playtime: 6.12  Epsilon:  0.55\nEpsiode: 800    Score: -7.08    Playtime: 7.98  Epsilon:  0.45\nEpsiode: 1000   Score: -1.87    Playtime: 10.65 Epsilon:  0.37\nEpsiode: 1200   Score: 28.23    Playtime: 10.16 Epsilon:  0.30\nEpsiode: 1400   Score: 61.38    Playtime: 6.62  Epsilon:  0.25\nEpsiode: 1600   Score: 66.42    Playtime: 5.98  Epsilon:  0.20\nEpsiode: 1800   Score: 74.94    Playtime: 6.26  Epsilon:  0.17\nEpsiode: 2000   Score: 75.29    Playtime: 5.91  Epsilon:  0.14\nEpsiode: 2200   Score: 77.24    Playtime: 6.16  Epsilon:  0.11\nEpsiode: 2400   Score: 86.1     Playtime: 6.1   Epsilon:  0.09\nEpsiode: 2600   Score: 83.81    Playtime: 6.19  Epsilon:  0.07\nEpsiode: 2800   Score: 87.27    Playtime: 6.03  Epsilon:  0.06\nEpsiode: 3000   Score: 86.1     Playtime: 6.1   Epsilon:  0.05\nEpsiode: 3200   Score: 87.37    Playtime: 5.93  Epsilon:  0.04\nEpsiode: 3400   Score: 93.68    Playtime: 6.22  Epsilon:  0.03\nEpsiode: 3600   Score: 90.58    Playtime: 6.02  Epsilon:  0.03\nEpsiode: 3800   Score: 92.77    Playtime: 6.03  Epsilon:  0.02\nEpsiode: 4000   Score: 93.79    Playtime: 6.11  Epsilon:  0.02\nEpsiode: 4200   Score: 94.88    Playtime: 6.12  Epsilon:  0.01\nEpsiode: 4400   Score: 92.85    Playtime: 5.95  Epsilon:  0.01\nEpsiode: 4600   Score: 94.96    Playtime: 6.04  Epsilon:  0.01\nEpsiode: 4800   Score: 94.92    Playtime: 6.08  Epsilon:  0.01\nEpsiode: 5000   Score: 93.9     Playtime: 6.0   Epsilon:  0.01\n\n\n\nstates = [np.array([0,0])] + agent.next_states[-agent.playtimes[-1]:] \nshow(states)\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect"
  }
]